{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==== COLAB CLEAN + LOCKED SETUP (run FIRST) ====\n",
        "import pathlib, textwrap, pkgutil, os\n",
        "\n",
        "# 0) Remove conflicting versions if they slipped in\n",
        "!pip uninstall -y pillow pandas >/dev/null 2>&1\n",
        "\n",
        "# 1) Create a constraints file with the versions that work with Docling + Colab\n",
        "CONSTRAINTS = \"/content/constraints.txt\"\n",
        "pathlib.Path(CONSTRAINTS).write_text(textwrap.dedent(\"\"\"\n",
        "pillow==11.3.0\n",
        "pandas==2.2.2\n",
        "jedi>=0.18\n",
        "\"\"\").strip()+\"\\n\")\n",
        "\n",
        "# 2) Make pip ALWAYS use these pins for the rest of the session\n",
        "%env PIP_CONSTRAINT=/content/constraints.txt\n",
        "\n",
        "# 3) Install the pins first\n",
        "!pip install -qU pillow==11.3.0 pandas==2.2.2 \"jedi>=0.18\"\n",
        "\n",
        "# 4) Install your stack under the constraints (so nothing upgrades Pillow/pandas)\n",
        "!pip install -qU --no-warn-conflicts \\\n",
        "  -c /content/constraints.txt \\\n",
        "  docling==2.57.0 \\\n",
        "  \"docling-core[chunking]\" \\\n",
        "  transformers \\\n",
        "  llama-index-core \\\n",
        "  llama-index-readers-docling \\\n",
        "  llama-index-node-parser-docling \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-vector-stores-milvus \\\n",
        "  sentence-transformers \\\n",
        "  tabulate \\\n",
        "  pylatexenc latex2mathml  # NEW (LaTeX): light utils for TeX cleaning/optional MathML\n",
        "\n",
        "# 5) Sanity check\n",
        "import PIL, pandas, pkgutil\n",
        "print(\"Pillow:\", PIL.__version__)\n",
        "print(\"pandas:\", pandas.__version__)\n",
        "print(\"jedi present:\", \"jedi\" in {m.name for m in pkgutil.iter_modules()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSfZbrB6EUoM",
        "outputId": "30291022-30c3-40f9-c66b-e0c75eb4bad9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PIP_CONSTRAINT=/content/constraints.txt\n",
            "Pillow: 11.3.0\n",
            "pandas: 2.2.2\n",
            "jedi present: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: installs\n",
        "%pip install -qU pip\n",
        "\n",
        "# Core parsing + chunking\n",
        "%pip install -qU docling docling-core transformers\n",
        "\n",
        "# RAG stack\n",
        "%pip install -qU llama-index-core \\\n",
        "                 llama-index-readers-docling \\\n",
        "                 llama-index-node-parser-docling \\\n",
        "                 llama-index-embeddings-huggingface \\\n",
        "                 llama-index-vector-stores-milvus\n",
        "\n",
        "# Utils\n",
        "%pip install -qU pillow pandas tabulate sentence-transformers pylatexenc latex2mathml  # NEW (LaTeX)\n",
        "# (Optional) Other vector stores:\n",
        "# %pip install -qU qdrant-client chromadb weaviate-client\n"
      ],
      "metadata": {
        "id": "JgYeCALKEV-T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "\n",
        "input_path = Path(\"/IVMSP-08-ImgMatch-Notes-2.pdf\")  # <- your file\n",
        "\n",
        "pdf_opts = PdfPipelineOptions()\n",
        "pdf_opts.images_scale = 2.0\n",
        "pdf_opts.generate_page_images = True\n",
        "pdf_opts.generate_picture_images = True\n",
        "\n",
        "converter = DocumentConverter(format_options={\n",
        "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)\n",
        "})\n",
        "\n",
        "conv_res = converter.convert(input_path)\n",
        "dl_doc = conv_res.document\n",
        "doc_stem = conv_res.input.file.stem\n",
        "\n",
        "print(\"Parsed pages:\", len(dl_doc.pages))\n",
        "print(\"Tables:\", len(dl_doc.tables), \"| Pictures:\", len(dl_doc.pictures))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL2NiJQ2EYuP",
        "outputId": "896e7c11-dbff-417e-bf76-3abbed88c74d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-10-20 10:37:26,308 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:26,318 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:27,795 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:28,017 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:28,019 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:28,321 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:28,322 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:29,234 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:29,262 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:29,264 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:29,394 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:29,396 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:30,133 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:30,526 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:30,528 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:49,785 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/resources/fonts/FZYTK.TTF\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:50,556 [RapidOCR] download_file.py:82: Download size: 3.09MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-20 10:37:50,641 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/FZYTK.TTF\u001b[0m\n",
            "\u001b[33m[WARNING] 2025-10-20 10:37:53,124 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "WARNING:docling.models.rapid_ocr_model:RapidOCR returned empty result!\n",
            "\u001b[33m[WARNING] 2025-10-20 10:37:54,338 [RapidOCR] main.py:123: The text detection result is empty\u001b[0m\n",
            "WARNING:docling.models.rapid_ocr_model:RapidOCR returned empty result!\n",
            "WARNING:docling.models.rapid_ocr_model:RapidOCR returned empty result!\n",
            "WARNING:docling.models.rapid_ocr_model:RapidOCR returned empty result!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed pages: 5\n",
            "Tables: 1 | Pictures: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
        "import pandas as pd\n",
        "\n",
        "# Create asset folders\n",
        "assets_base = Path(\"/content/repo/assets\")\n",
        "pages_dir    = assets_base / \"pages\"\n",
        "figs_dir     = assets_base / \"figures\"\n",
        "tables_dir   = assets_base / \"tables\"\n",
        "formulas_dir = assets_base / \"formulas\"\n",
        "text_dir     = Path(\"/content/repo/text/markdown\")\n",
        "json_dir     = Path(\"/content/repo/text/json\")\n",
        "latex_dir    = Path(\"/content/repo/text/latex\")  # NEW (LaTeX)\n",
        "\n",
        "for d in [pages_dir, figs_dir, tables_dir, formulas_dir, text_dir, json_dir, latex_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2A) Save page images\n",
        "for page_no, page in dl_doc.pages.items():\n",
        "    out = pages_dir / f\"{doc_stem}_page_{page_no}.png\"\n",
        "    page.image.pil_image.save(out, \"PNG\")\n",
        "\n",
        "# 2B) Export figures and table preview images\n",
        "fig_ix, tbl_ix = 0, 0\n",
        "for element, _lvl in dl_doc.iterate_items():\n",
        "    if isinstance(element, PictureItem):\n",
        "        fig_ix += 1\n",
        "        out = figs_dir / f\"{doc_stem}_fig_{fig_ix}.png\"\n",
        "        element.get_image(dl_doc).save(out, \"PNG\")\n",
        "    if isinstance(element, TableItem):\n",
        "        tbl_ix += 1\n",
        "        out = tables_dir / f\"{doc_stem}_table_{tbl_ix}.png\"\n",
        "        element.get_image(dl_doc).save(out, \"PNG\")\n",
        "\n",
        "# 2C) Export tables as CSV + HTML\n",
        "for k, t in enumerate(dl_doc.tables, start=1):\n",
        "    df = t.export_to_dataframe()\n",
        "    df.to_csv(tables_dir / f\"{doc_stem}_table_{k}.csv\", index=False)\n",
        "    (tables_dir / f\"{doc_stem}_table_{k}.html\").write_text(t.export_to_html(doc=dl_doc))\n",
        "\n",
        "# 2D) Human-friendly exports with referenced images\n",
        "md_ref = text_dir / f\"{doc_stem}_with_image_refs.md\"\n",
        "html_ref = text_dir / f\"{doc_stem}_with_image_refs.html\"\n",
        "dl_doc.save_as_markdown(md_ref, image_mode=ImageRefMode.REFERENCED)\n",
        "dl_doc.save_as_html(html_ref, image_mode=ImageRefMode.REFERENCED)\n",
        "\n",
        "# 2E) Save native Docling JSON\n",
        "(json_dir / f\"{doc_stem}.json\").write_text(dl_doc.model_dump_json())\n",
        "\n",
        "# 2F) Prepare LaTeX outputs (files created in Cell 5)\n",
        "latex_jsonl_path = latex_dir / f\"{doc_stem}_formulas.jsonl\"   # NEW (LaTeX)\n",
        "latex_md_path    = latex_dir / f\"{doc_stem}_formulas.md\"      # NEW (LaTeX)\n",
        "\n",
        "print(\"Assets exported under:\", assets_base)\n",
        "print(\"LaTeX paths prepared:\", latex_jsonl_path, latex_md_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50xWIhfGEbFo",
        "outputId": "0cd63966-b24c-476a-906f-975b6afb8fbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n",
            "WARNING:docling_core.transforms.serializer.html:Could not parse formula with MathML\n",
            "WARNING:docling_core.transforms.serializer.html:Could not parse formula with MathML\n",
            "WARNING:docling_core.transforms.serializer.html:Could not parse formula with MathML\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assets exported under: /content/repo/assets\n",
            "LaTeX paths prepared: /content/repo/text/latex/IVMSP-08-ImgMatch-Notes-2_formulas.jsonl /content/repo/text/latex/IVMSP-08-ImgMatch-Notes-2_formulas.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5 (Hybrid Math Detector): true formulas + mathy text + picture fallback =====\n",
        "from typing import Iterable, Tuple, Optional, List, Dict\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from docling_core.types.doc import DocItemLabel, TextItem, PictureItem\n",
        "try:\n",
        "    from docling_core.types.doc import FormulaItem\n",
        "except Exception:\n",
        "    FormulaItem = tuple()  # sentinel if class not available\n",
        "\n",
        "import io, hashlib, json, re\n",
        "\n",
        "# ---- tuning knobs ----\n",
        "STRICT_SKIP_NO_BBOX   = True     # don't crop full pages (prevents slide duplicates)\n",
        "FALLBACK_LAST_PAGES   = 6        # slides often put formulas at the end\n",
        "FALLBACK_MIN_W, FALLBACK_MIN_H = 80, 40  # ignore tiny logos/icons\n",
        "TEXT_MATH_MIN_LEN     = 3        # don't flag ultra-short tokens as formula\n",
        "# Recognize TeX-ish and math-y content in text lines\n",
        "MATHY_REGEX = re.compile(\n",
        "    r\"(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.+?\\\\\\)|\"        # TeX delimiters\n",
        "    r\"\\\\frac|\\\\sum|\\\\int|\\\\lim|\\\\log|\\\\sin|\\\\cos|\\\\tan|\\\\alpha|\\\\beta|\\\\gamma|\\\\infty|\\\\cdot|\\\\times|\\\\sqrt|\"\n",
        "    r\"[∑∏∫∞≈≃≅≡≤≥≪≫±×·÷∂∇√⊂⊆⊃⊇∈∉∩∪∧∨¬⇒⇔→←↔≈≃≅≡≤≥⊥∥]\"\n",
        "    r\"|[=^_]{1,}.*[0-9A-Za-z])\"\n",
        ")\n",
        "\n",
        "def _bbox_box(item) -> Optional[tuple]:\n",
        "    bbox = getattr(item, \"bbox\", None)\n",
        "    if bbox is None:\n",
        "        return None\n",
        "    try:\n",
        "        return tuple(bbox.to_box())\n",
        "    except Exception:\n",
        "        try:\n",
        "            return tuple(bbox)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def _page_no_of_item(doc, item) -> Optional[int]:\n",
        "    for attr in (\"page_no\", \"page_number\", \"page_idx\", \"page_index\"):\n",
        "        v = getattr(item, attr, None)\n",
        "        if v is not None and v in doc.pages:\n",
        "            return v\n",
        "    page_obj = getattr(item, \"page\", None)\n",
        "    if page_obj is not None:\n",
        "        v = getattr(page_obj, \"page_no\", None)\n",
        "        if v is not None and v in doc.pages:\n",
        "            return v\n",
        "    # bbox containment fallback\n",
        "    box = _bbox_box(item)\n",
        "    if box is not None:\n",
        "        x0, y0, x1, y1 = box\n",
        "        for pn, page in doc.pages.items():\n",
        "            W, H = page.image.pil_image.size\n",
        "            if 0 <= x0 < x1 <= W and 0 <= y0 < y1 <= H:\n",
        "                return pn\n",
        "    return None\n",
        "\n",
        "def _png_md5(img: Image.Image) -> str:\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()\n",
        "\n",
        "def _wrap_tex(tex: str) -> Optional[str]:\n",
        "    tex = (tex or \"\").strip()\n",
        "    if not tex:\n",
        "        return None\n",
        "    if tex.startswith(\"$$\") or tex.startswith(\"\\\\[\") or (tex.startswith(\"$\") and tex.endswith(\"$\")):\n",
        "        return tex\n",
        "    return f\"$$\\n{tex}\\n$$\"\n",
        "\n",
        "def _iter_true_formula_items(doc):\n",
        "    for item, _lvl in doc.iterate_items():\n",
        "        if (FormulaItem and isinstance(item, FormulaItem)) or \\\n",
        "           (isinstance(item, TextItem) and item.label == DocItemLabel.FORMULA):\n",
        "            yield item\n",
        "\n",
        "def _iter_mathy_text_items(doc):\n",
        "    \"\"\"Pick up equations typeset as text (common in some PDFs/slide exports).\"\"\"\n",
        "    for item, _lvl in doc.iterate_items():\n",
        "        if not isinstance(item, TextItem):\n",
        "            continue\n",
        "        s = (getattr(item, \"text\", None) or \"\").strip()\n",
        "        if len(s) < TEXT_MATH_MIN_LEN:\n",
        "            continue\n",
        "        if MATHY_REGEX.search(s):\n",
        "            yield item\n",
        "\n",
        "def _iter_picture_fallback(doc, tail_pages):\n",
        "    for item, _lvl in doc.iterate_items():\n",
        "        if not isinstance(item, PictureItem):\n",
        "            continue\n",
        "        pn = _page_no_of_item(doc, item)\n",
        "        if pn not in tail_pages:\n",
        "            continue\n",
        "        box = _bbox_box(item)\n",
        "        if pn is None or box is None:\n",
        "            continue\n",
        "        x0, y0, x1, y1 = box\n",
        "        if (x1 - x0) < FALLBACK_MIN_W or (y1 - y0) < FALLBACK_MIN_H:\n",
        "            continue\n",
        "        yield item\n",
        "\n",
        "def _crop_by_item(doc, item) -> Optional[Tuple[Image.Image, int, Optional[tuple]]]:\n",
        "    pn = _page_no_of_item(doc, item)\n",
        "    box = _bbox_box(item)\n",
        "    if STRICT_SKIP_NO_BBOX and (pn is None or box is None):\n",
        "        return None\n",
        "    page_img = doc.pages[pn].image.pil_image if pn is not None else next(iter(doc.pages.values())).image.pil_image\n",
        "    crop = page_img if box is None else page_img.crop(box)\n",
        "    return crop, pn, box\n",
        "\n",
        "# Accumulators\n",
        "records: List[Dict] = []\n",
        "seen_md5 = set()\n",
        "\n",
        "# 1) True formula items\n",
        "for item in _iter_true_formula_items(dl_doc):\n",
        "    out = _crop_by_item(dl_doc, item)\n",
        "    if out is None:\n",
        "        continue\n",
        "    crop, pn, box = out\n",
        "    md5 = _png_md5(crop)\n",
        "    if md5 in seen_md5:\n",
        "        continue\n",
        "    seen_md5.add(md5)\n",
        "    idx = len(seen_md5)\n",
        "\n",
        "    # Try to read any latex-ish field provided by docling\n",
        "    raw = None\n",
        "    for cand in (\"latex\", \"tex\", \"text\", \"content\"):\n",
        "        raw = getattr(item, cand, None)\n",
        "        if raw:\n",
        "            break\n",
        "    tex = _wrap_tex(raw)\n",
        "\n",
        "    path = formulas_dir / f\"{doc_stem}_formula_{idx}.png\"\n",
        "    crop.save(path, \"PNG\")\n",
        "    records.append({\n",
        "        \"id\": f\"{doc_stem}_f{idx}\",\n",
        "        \"page\": pn,\n",
        "        \"bbox\": list(box) if box else None,\n",
        "        \"tex\": tex,\n",
        "        \"mathml\": None,   # (optional OCR step can fill later)\n",
        "        \"image_path\": str(path),\n",
        "        \"context_prev\": getattr(getattr(item, \"prev\", None), \"text\", None),\n",
        "        \"context_next\": getattr(getattr(item, \"next\", None), \"text\", None),\n",
        "        \"source\": \"formula_item\",\n",
        "    })\n",
        "\n",
        "# 2) Mathy TextItems (heuristic)\n",
        "for item in _iter_mathy_text_items(dl_doc):\n",
        "    out = _crop_by_item(dl_doc, item)\n",
        "    if out is None:\n",
        "        continue\n",
        "    crop, pn, box = out\n",
        "    md5 = _png_md5(crop)\n",
        "    if md5 in seen_md5:\n",
        "        continue\n",
        "    seen_md5.add(md5)\n",
        "    idx = len(seen_md5)\n",
        "\n",
        "    s = (getattr(item, \"text\", None) or \"\").strip()\n",
        "    tex = _wrap_tex(s)\n",
        "\n",
        "    path = formulas_dir / f\"{doc_stem}_formula_{idx}.png\"\n",
        "    crop.save(path, \"PNG\")\n",
        "    records.append({\n",
        "        \"id\": f\"{doc_stem}_f{idx}\",\n",
        "        \"page\": pn,\n",
        "        \"bbox\": list(box) if box else None,\n",
        "        \"tex\": tex,       # comes from text; good enough for retrieval/rendering\n",
        "        \"mathml\": None,\n",
        "        \"image_path\": str(path),\n",
        "        \"context_prev\": getattr(getattr(item, \"prev\", None), \"text\", None),\n",
        "        \"context_next\": getattr(getattr(item, \"next\", None), \"text\", None),\n",
        "        \"source\": \"text_heuristic\",\n",
        "    })\n",
        "\n",
        "# 3) Picture fallback on tail pages (for rasterized equations at end of slides)\n",
        "if not records:\n",
        "    pages_sorted = sorted(dl_doc.pages.keys())\n",
        "    tail_pages = pages_sorted[-FALLBACK_LAST_PAGES:] if len(pages_sorted) >= FALLBACK_LAST_PAGES else pages_sorted\n",
        "    for item in _iter_picture_fallback(dl_doc, tail_pages):\n",
        "        out = _crop_by_item(dl_doc, item)\n",
        "        if out is None:\n",
        "            continue\n",
        "        crop, pn, box = out\n",
        "        md5 = _png_md5(crop)\n",
        "        if md5 in seen_md5:\n",
        "            continue\n",
        "        seen_md5.add(md5)\n",
        "        idx = len(seen_md5)\n",
        "        path = formulas_dir / f\"{doc_stem}_formula_{idx}.png\"\n",
        "        crop.save(path, \"PNG\")\n",
        "        records.append({\n",
        "            \"id\": f\"{doc_stem}_f{idx}\",\n",
        "            \"page\": pn,\n",
        "            \"bbox\": list(box) if box else None,\n",
        "            \"tex\": None,     # no TeX; can be filled by OCR in Cell 5.1\n",
        "            \"mathml\": None,\n",
        "            \"image_path\": str(path),\n",
        "            \"context_prev\": None,\n",
        "            \"context_next\": None,\n",
        "            \"source\": \"picture_fallback\",\n",
        "        })\n",
        "\n",
        "# Write JSONL + friendly MD index\n",
        "latex_jsonl = latex_dir / f\"{doc_stem}_formulas.jsonl\"\n",
        "latex_md    = latex_dir / f\"{doc_stem}_formulas.md\"\n",
        "with open(latex_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "    for rec in records:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "md_lines = [f\"# Formulas / candidates from `{doc_stem}`\\n\"]\n",
        "for j, rec in enumerate(records, start=1):\n",
        "    md_lines.append(f\"## {j}. Page {rec['page']} — {rec['source']}\")\n",
        "    md_lines.append(rec[\"tex\"] if rec[\"tex\"] else \"_(no TeX — picture)_\")\n",
        "    md_lines.append(f\"\\n![crop]({rec['image_path']})\\n\")\n",
        "Path(latex_md).write_text(\"\\n\".join(md_lines), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved {len(records)} unique formula/candidate crops.\")\n",
        "print(\"LaTeX JSONL:\", latex_jsonl)\n",
        "print(\"LaTeX MD:\", latex_md)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQONF8q-Edsb",
        "outputId": "e008a70c-032e-4ad6-bab8-89f2ff152cb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 0 unique formula/candidate crops.\n",
            "LaTeX JSONL: /content/repo/text/latex/IVMSP-08-ImgMatch-Notes-2_formulas.jsonl\n",
            "LaTeX MD: /content/repo/text/latex/IVMSP-08-ImgMatch-Notes-2_formulas.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5.1 (Optional): Try OCR (im2LaTeX) on picture_fallback crops to fill 'tex' =====\n",
        "import json, sys, traceback\n",
        "from pathlib import Path\n",
        "\n",
        "latex_jsonl = latex_dir / f\"{doc_stem}_formulas.jsonl\"\n",
        "tmp_jsonl   = latex_dir / f\"{doc_stem}_formulas_tmp.jsonl\"\n",
        "\n",
        "def _try_import_im2latex():\n",
        "    try:\n",
        "        # Two popular packages; try either\n",
        "        import pix2tex  # noqa\n",
        "        return \"pix2tex\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        import latex_ocr  # noqa\n",
        "        return \"latex_ocr\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "backend = _try_import_im2latex()\n",
        "if backend is None:\n",
        "    # Try to install a backend quickly\n",
        "    try:\n",
        "        # pix2tex is common; if install fails, we give up gracefully\n",
        "        %pip install -qU pix2tex\n",
        "        import pix2tex  # noqa\n",
        "        backend = \"pix2tex\"\n",
        "    except Exception:\n",
        "        print(\"⚠️ Could not import/install an im2LaTeX backend. Skipping OCR.\")\n",
        "        backend = None\n",
        "\n",
        "def _ocr_pix2tex(img_path: str) -> Optional[str]:\n",
        "    try:\n",
        "        from pix2tex.cli import LatexOCR\n",
        "        from PIL import Image\n",
        "        model = LatexOCR()\n",
        "        pred = model(Image.open(img_path))\n",
        "        tex = str(pred).strip()\n",
        "        return tex if tex else None\n",
        "    except Exception:\n",
        "        traceback.print_exc(file=sys.stdout)\n",
        "        return None\n",
        "\n",
        "def _ocr_one(image_path: str) -> Optional[str]:\n",
        "    if backend == \"pix2tex\":\n",
        "        return _ocr_pix2tex(image_path)\n",
        "    # Add other backends here if you prefer\n",
        "    return None\n",
        "\n",
        "updated = 0\n",
        "with open(latex_jsonl, \"r\", encoding=\"utf-8\") as fin, open(tmp_jsonl, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in fin:\n",
        "        rec = json.loads(line)\n",
        "        if rec.get(\"tex\") or rec.get(\"source\") != \"picture_fallback\" or backend is None:\n",
        "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            continue\n",
        "        guess = _ocr_one(rec[\"image_path\"])\n",
        "        if guess:\n",
        "            rec[\"tex\"] = f\"$$\\n{guess}\\n$$\"\n",
        "            updated += 1\n",
        "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "Path(latex_jsonl).unlink()\n",
        "Path(tmp_jsonl).rename(latex_jsonl)\n",
        "print(f\"OCR filled TeX for {updated} picture crops.\")\n"
      ],
      "metadata": {
        "id": "Lgv3_4mPPTcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05d9bdd-a7d5-4aec-fbe5-e9b7825cbb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OCR filled TeX for 0 picture crops.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.chunking import HybridChunker\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMB_MODEL_NAME = \"BAAI/bge-m3\"  # strong, multilingual text retrieval\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_NAME)\n",
        "\n",
        "chunker = HybridChunker(\n",
        "    tokenizer=tokenizer,\n",
        "    max_tokens=512,\n",
        "    merge_peers=True\n",
        ")\n",
        "\n",
        "chunks = list(chunker.chunk(dl_doc))\n",
        "print(\"Chunk count:\", len(chunks))\n",
        "print(chunks[0].text[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwyhWpYEEgsT",
        "outputId": "12555122-7530-4e7c-8bf3-1eb67bfc0e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk count: 2\n",
            "- Randomization adds statistical independence and decorrelates trees:\n",
            "- Averaging the votes of N statistically independent and identically distributed (i.i.d.) learners with accuracy > 50 % arbitrarily reduces the variance:\n",
            "<!-- formula-not-decoded -->\n",
            "- Note, however, that the i.i.d. assumption is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Milvus Lite extra for local DB URIs\n",
        "!pip install -qU -c /content/constraints.txt \"pymilvus[milvus_lite]>=2.4.7\"\n",
        "\n",
        "from pymilvus import connections\n",
        "connections.connect(alias=\"default\", uri=\"/content/milvus_docling.db\")\n",
        "print(\"Milvus Lite local DB: OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJgG10QbEkgS",
        "outputId": "50bcf275-c9d4-482f-975a-2bcac851e4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import DistributionNotFound, get_distribution\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milvus Lite local DB: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Fixed Cell 9: build text index, then insert formula docs (no DoclingNodeParser) =====\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, Document\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.node_parser.docling import DoclingNodeParser\n",
        "import json\n",
        "\n",
        "json_path = (json_dir / f\"{doc_stem}.json\")\n",
        "\n",
        "# 5A) Text embedding model\n",
        "text_embed = HuggingFaceEmbedding(model_name=EMB_MODEL_NAME)\n",
        "dim = len(text_embed.get_text_embedding(\"hello\"))\n",
        "\n",
        "# 5B) Milvus Lite (local file -> Lite)\n",
        "milvus_uri = \"/content/milvus_docling.db\"\n",
        "text_store = MilvusVectorStore(uri=milvus_uri, dim=dim, collection_name=\"text\", overwrite=True)\n",
        "storage_ctx = StorageContext.from_defaults(vector_store=text_store)\n",
        "\n",
        "# 5C) Docling-aware parsing → Nodes with page/bbox metadata (apply ONLY to Docling docs)\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)\n",
        "node_parser = DoclingNodeParser()\n",
        "\n",
        "docling_documents = reader.load_data(json_path)  # LlamaIndex Documents from Docling JSON\n",
        "\n",
        "# Build index from Docling docs WITH the DoclingNodeParser\n",
        "text_index = VectorStoreIndex.from_documents(\n",
        "    documents=docling_documents,\n",
        "    transformations=[node_parser],\n",
        "    storage_context=storage_ctx,\n",
        "    embed_model=text_embed,\n",
        ")\n",
        "\n",
        "# 5D) Load TeX formula docs produced in Cell 5 and insert them WITHOUT DoclingNodeParser\n",
        "formula_docs = []\n",
        "with open(latex_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        tex = (rec.get(\"tex\") or \"\").strip()\n",
        "        ctx_prev = (rec.get(\"context_prev\") or \"\").strip()\n",
        "        ctx_next = (rec.get(\"context_next\") or \"\").strip()\n",
        "        text_blob = \"\\n\".join([s for s in [ctx_prev, tex, ctx_next] if s])\n",
        "\n",
        "        formula_docs.append(\n",
        "            Document(\n",
        "                text=text_blob if text_blob else tex,\n",
        "                metadata={\n",
        "                    \"type\": \"formula\",\n",
        "                    \"is_formula\": True,\n",
        "                    \"page_number\": rec.get(\"page\"),\n",
        "                    \"bbox\": rec.get(\"bbox\"),\n",
        "                    \"image_path\": rec.get(\"image_path\"),\n",
        "                    \"id\": rec.get(\"id\"),\n",
        "                    \"file_name\": f\"{doc_stem}.pdf\",\n",
        "                },\n",
        "            )\n",
        "        )\n",
        "# Insert plain formula docs directly (no transformations)\n",
        "if formula_docs:\n",
        "    # Reuse the SAME storage_ctx (same Milvus collection) and same embed model.\n",
        "    # No DoclingNodeParser here.\n",
        "    _ = VectorStoreIndex.from_documents(\n",
        "        documents=formula_docs,\n",
        "        storage_context=storage_ctx,\n",
        "        embed_model=text_embed,\n",
        "        # no transformations -> they're plain text docs\n",
        "    )\n",
        "\n",
        "text_retriever = text_index.as_retriever(similarity_top_k=5)\n",
        "print(f\"Indexed {len(docling_documents)} Docling docs and inserted {len(formula_docs)} TeX formula docs.\")\n",
        "\n",
        "# Load per-path metadata for captions\n",
        "meta_by_path = {}\n",
        "with open(latex_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        meta_by_path[rec[\"image_path\"]] = {\n",
        "            \"tex\": (rec.get(\"tex\") or \"\").strip().replace(\"\\n\",\" \"),\n",
        "            \"source\": rec.get(\"source\",\"\"),\n",
        "        }\n",
        "\n",
        "image_docs = []\n",
        "for p in sorted((assets_base / \"figures\").glob(\"*.png\")):\n",
        "    image_docs.append(image_doc(p, f\"Figure from {doc_stem}\", {\"type\": \"figure\"}))\n",
        "for p in sorted((assets_base / \"formulas\").glob(\"*.png\")):\n",
        "    meta = meta_by_path.get(str(p), {\"tex\":\"\", \"source\":\"\"})\n",
        "    tex_snip = meta[\"tex\"][:256]\n",
        "    src = meta[\"source\"]\n",
        "    cap = f\"Formula crop ({src}) from {doc_stem}\" + (f\". TeX: {tex_snip}\" if tex_snip else \"\")\n",
        "    image_docs.append(image_doc(p, cap, {\"type\": \"formula\", \"is_formula\": True, \"source\": src}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "2_oy446DEm31",
        "outputId": "5d8aac41-9405-40c3-a485-ee04897dd86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 1 Docling docs and inserted 0 TeX formula docs.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_doc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-89719130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mimage_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_base\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"figures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mimage_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Figure from {doc_stem}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"figure\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_base\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"formulas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_by_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tex\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_doc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, StorageContext, Document\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "IMG_EMB_MODEL = \"clip-ViT-B-32\"  # sentence-transformers alias\n",
        "img_embed = HuggingFaceEmbedding(model_name=IMG_EMB_MODEL)\n",
        "img_dim = len(img_embed.get_text_embedding(\"x\"))  # CLIP uses same dim for text encoder\n",
        "\n",
        "image_store = MilvusVectorStore(uri=milvus_uri, dim=img_dim, collection_name=\"images\", overwrite=True)\n",
        "image_storage = StorageContext.from_defaults(vector_store=image_store)\n",
        "\n",
        "def image_doc(path: Path, caption: str, meta: dict):\n",
        "    return Document(text=caption, metadata={\"image_path\": str(path), **meta})\n",
        "\n",
        "# Load TeX per-id for formula captions\n",
        "import json\n",
        "tex_by_path = {}\n",
        "with open(latex_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        p = rec.get(\"image_path\")\n",
        "        t = (rec.get(\"tex\") or \"\").strip().replace(\"\\n\", \" \")\n",
        "        tex_by_path[p] = t\n",
        "\n",
        "image_docs = []\n",
        "for p in sorted((assets_base / \"figures\").glob(\"*.png\")):\n",
        "    image_docs.append(image_doc(p, f\"Figure from {doc_stem}\", {\"type\": \"figure\"}))\n",
        "for p in sorted((assets_base / \"formulas\").glob(\"*.png\")):\n",
        "    tex = tex_by_path.get(str(p), \"\")\n",
        "    cap = f\"Formula crop from {doc_stem}. TeX: {tex[:256]}\" if tex else f\"Formula crop from {doc_stem}\"\n",
        "    image_docs.append(image_doc(p, cap, {\"type\": \"formula\", \"is_formula\": True}))\n",
        "\n",
        "image_index = VectorStoreIndex.from_documents(image_docs, storage_context=image_storage, embed_model=img_embed)\n",
        "image_retriever = image_index.as_retriever(similarity_top_k=5)\n",
        "print(f\"Image docs -> figures: {len([d for d in image_docs if d.metadata.get('type')=='figure'])}, formulas: {len([d for d in image_docs if d.metadata.get('type')=='formula'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTloxPs8Eo-X",
        "outputId": "e6382ff1-a7eb-4d01-ee17-5b8d598be602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image docs -> figures: 4, formulas: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def _is_mathy(q: str) -> bool:\n",
        "    # Simple heuristic: LaTeX delimiters or common commands\n",
        "    return bool(re.search(r\"(\\$|\\\\frac|\\\\sum|\\\\int|\\\\alpha|\\\\beta|\\\\gamma|\\\\infty|\\\\begin\\{equation\\})\", q))\n",
        "\n",
        "def fused_retrieve(query: str, k: int = 8):\n",
        "    text_hits  = text_retriever.retrieve(query)\n",
        "    image_hits = image_retriever.retrieve(query)\n",
        "\n",
        "    # naive interleave\n",
        "    fused = []\n",
        "    for a, b in zip(text_hits, image_hits):\n",
        "        fused.extend([a, b])\n",
        "\n",
        "    fused = fused[:k]\n",
        "\n",
        "    # If mathy, bubble up formula items\n",
        "    if _is_mathy(query):\n",
        "        def score_key(h):\n",
        "            meta = getattr(h, \"metadata\", {}) or {}\n",
        "            return (1 if meta.get(\"is_formula\") else 0, )  # formulas first\n",
        "        fused = sorted(fused, key=score_key, reverse=True)\n",
        "\n",
        "    return fused, text_hits, image_hits\n",
        "\n",
        "query = r\"Where is the main theorem defined, and what does Figure 2 illustrate? Show me the formula for $\\alpha=\\frac{x}{y}$.\"\n",
        "fused, text_hits, image_hits = fused_retrieve(query)\n",
        "\n",
        "print(\"Top text nodes:\")\n",
        "for h in text_hits[:3]:\n",
        "    meta = getattr(h, \"metadata\", {}) or {}\n",
        "    print(\"-\", h.get_content()[:140].replace(\"\\n\",\" \"),\n",
        "          \"| meta:\", {k:meta.get(k) for k in [\"page_number\",\"bbox\",\"type\",\"file_name\",\"is_formula\"]})\n",
        "\n",
        "print(\"\\nTop images:\")\n",
        "for h in image_hits[:3]:\n",
        "    meta = getattr(h, \"metadata\", {}) or {}\n",
        "    print(\"-\", meta.get(\"type\"), meta.get(\"image_path\"), \"| is_formula:\", meta.get(\"is_formula\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPfDqImqErrN",
        "outputId": "5b49cc82-738c-4882-e36f-eaff2adcb980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top text nodes:\n",
            "- <!-- formula-not-decoded --> | meta: {'page_number': None, 'bbox': None, 'type': None, 'file_name': None, 'is_formula': None}\n",
            "- <!-- formula-not-decoded --> | meta: {'page_number': None, 'bbox': None, 'type': None, 'file_name': None, 'is_formula': None}\n",
            "- <!-- formula-not-decoded --> | meta: {'page_number': None, 'bbox': None, 'type': None, 'file_name': None, 'is_formula': None}\n",
            "\n",
            "Top images:\n",
            "- figure /content/repo/assets/figures/QoSiC_4_AnalyticalModeling_fig_3.png | is_formula: None\n",
            "- figure /content/repo/assets/figures/QoSiC_4_AnalyticalModeling_fig_4.png | is_formula: None\n",
            "- figure /content/repo/assets/figures/QoSiC_4_AnalyticalModeling_fig_2.png | is_formula: None\n"
          ]
        }
      ]
    }
  ]
}